{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "import re\n",
    "import string\n",
    "import gensim\n",
    "#nltk.download()\n",
    "#python -m nltk.downloader all - from command line\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.nmf import Nmf \n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# stop_words = stopwords.words('english')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# texts = []\n",
    "# lemmatized_tokens_list=[]\n",
    "# for i in doc_set:\n",
    "#     raw = i.lower()\n",
    "#     tokens = tokenizer.tokenize(raw)\n",
    "#     stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "#     lemmatized_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "#     lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "#     lemmatized_tokens_combined = ' '.join([lemmatizer.lemmatize(i) for i in stopped_tokens])\n",
    "#     texts.append(lemmatized_tokens_combined)\n",
    "\n",
    "# print(texts)\n",
    "# print(lemmatized_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spends', 'lot', 'time', 'driving', 'brother', 'around', 'baseball', 'practice'], ['health', 'expert', 'suggest', 'driving', 'may', 'cause', 'increased', 'tension', 'blood', 'pressure'], ['often', 'feel', 'pressure', 'perform', 'well', 'school', 'mother', 'never', 'seems', 'drive', 'brother', 'better'], ['health', 'professional', 'say', 'brocolli', 'good', 'health']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts = []\n",
    "for i in doc_set:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "    texts.append(lemmatized_tokens)\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vectorization\n",
    "# cv_vect = CountVectorizer(max_df=0.8, min_df=2)\n",
    "# doc_term_matrix = cv_vect.fit_transform(texts)\n",
    "\n",
    "# #Model building\n",
    "# nmf = NMF(n_components=2, random_state=42)\n",
    "# nmf.fit(doc_term_matrix)\n",
    "\n",
    "# #fetches 10 words with highest probabilities for all the five topics\n",
    "# for i,topic in enumerate(nmf.components_):\n",
    "#     print(f'Top 3 words for topic #{i}:')\n",
    "#     print([cv_vect.get_feature_names()[i] for i in topic.argsort()[-3:]])\n",
    "\n",
    "# #saving results\n",
    "# topic_values = nmf.transform(doc_term_matrix)\n",
    "# topic_values.shape\n",
    "# print('\\n')\n",
    "# print('Topics Associated With Documents')\n",
    "# for i in range(len(doc_set)):\n",
    "#     print(doc_set[i] + ' - ' + 'Topic ' + str(topic_values.argmax(axis=1)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n",
      "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)], [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(1, 1), (5, 1), (19, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)], [(0, 1), (3, 1), (16, 2), (31, 1), (32, 1)]]\n",
      "[(0, '0.202*\"good\" + 0.202*\"brocolli\" + 0.161*\"eat\" + 0.084*\"mother\"'), (1, '0.069*\"pressure\" + 0.061*\"brother\" + 0.061*\"mother\" + 0.059*\"driving\"')]\n",
      "\n",
      "Coherence Score:  0.2896196628606057\n",
      "\n",
      "Coherence Score:  -14.376210806203396\n"
     ]
    }
   ],
   "source": [
    "#Vectorization\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(dictionary)\n",
    "print(bow_corpus)\n",
    "\n",
    "#Model building\n",
    "nmfmodel_bow_corpus = Nmf(bow_corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "# corpus (iterable of list of (int, float) or csc_matrix with the shape (n_tokens, n_documents), optional) – Training corpus. \n",
    "# Can be either iterable of documents, which are lists of (word_id, word_count) or a sparse csc matrix of BOWs for each document. \n",
    "# If not specified, the model is left uninitialized (presumably, to be trained later with self.train()).\n",
    "# num_topics (int, optional) – Number of topics to extract.\n",
    "# id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) – Mapping from word IDs to words. \n",
    "# It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "# chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "# passes (int, optional) – Number of full passes over the training corpus. Leave at default passes=1 if your input is an iterator.\n",
    "# kappa (float, optional) – Gradient descent step size. Larger value makes the model train faster\n",
    "# but could lead to non-convergence if set too large.\n",
    "# minimum_probability – If normalize is True, topics with smaller probabilities are filtered out. \n",
    "# If normalize is False, topics with smaller factors are filtered out. If set to None, a value of 1e-8 is used to prevent 0s.\n",
    "# w_max_iter (int, optional) – Maximum number of iterations to train W per each batch.\n",
    "# w_stop_condition (float, optional) – If error difference gets less than that, training of W stops for the current batch.\n",
    "# h_max_iter (int, optional) – Maximum number of iterations to train h per each batch.\n",
    "# h_stop_condition (float) – If error difference gets less than that, training of h stops for the current batch.\n",
    "# eval_every (int, optional) – Number of batches after which l2 norm of (v - Wh) is computed. Decreases performance if set too low.\n",
    "# normalize (bool or None, optional) – Whether to normalize the result. Allows for estimation of perplexity, coherence, e.t.c.\n",
    "# random_state ({np.random.RandomState, int}, optional) – Seed for random generator. Needed for reproducibility.\n",
    "\n",
    "\n",
    "print(nmfmodel_bow_corpus.print_topics(num_topics=2, num_words=4))\n",
    "\n",
    "#Evaluating model\n",
    "coherence_model_lda = models.CoherenceModel(model=nmfmodel_bow_corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = models.CoherenceModel(model=nmfmodel_bow_corpus, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Vectorization\n",
    "# tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2)\n",
    "# doc_term_matrix = tfidf_vect.fit_transform(texts)\n",
    "\n",
    "# #Model building\n",
    "# nmf = NMF(n_components=2, random_state=42)\n",
    "# nmf.fit(doc_term_matrix)\n",
    "\n",
    "# #fetches 10 words with highest probabilities for all the five topics\n",
    "# for i,topic in enumerate(nmf.components_):\n",
    "#     print(f'Top 3 words for topic #{i}:')\n",
    "#     print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-3:]])\n",
    "\n",
    "# #saving results\n",
    "# topic_values = nmf.transform(doc_term_matrix)\n",
    "# topic_values.shape\n",
    "# print('\\n')\n",
    "# print('Topics Associated With Documents')\n",
    "# for i in range(len(doc_set)):\n",
    "#     print(doc_set[i] + ' - ' + 'Topic ' + str(topic_values.argmax(axis=1)[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['brocolli', 'brother', 'eat', 'good', 'like']...)\n",
      "[(0, 0.40784451109112935), (1, 0.11368521994734913), (2, 0.7163669735975946), (3, 0.40784451109112935), (4, 0.3581834867987973), (5, 0.11368521994734913)]\n",
      "[(1, 0.12424759593709131), (5, 0.12424759593709131), (6, 0.3914619434234833), (7, 0.3914619434234833), (8, 0.2228684610131362), (9, 0.3914619434234833), (10, 0.3914619434234833), (11, 0.3914619434234833), (12, 0.3914619434234833)]\n",
      "[(8, 0.2016345105176491), (13, 0.35416512946544426), (14, 0.35416512946544426), (15, 0.35416512946544426), (16, 0.2016345105176491), (17, 0.35416512946544426), (18, 0.35416512946544426), (19, 0.2016345105176491), (20, 0.35416512946544426), (21, 0.35416512946544426)]\n",
      "[(1, 0.10283764444679584), (5, 0.10283764444679584), (19, 0.18446447498008656), (22, 0.32400646345397865), (23, 0.32400646345397865), (24, 0.32400646345397865), (25, 0.32400646345397865), (26, 0.32400646345397865), (27, 0.32400646345397865), (28, 0.32400646345397865), (29, 0.32400646345397865), (30, 0.32400646345397865)]\n",
      "[(0, 0.2866473576676298), (3, 0.2866473576676298), (16, 0.5732947153352596), (31, 0.5034877128853272), (32, 0.5034877128853272)]\n",
      "[[(0, 2), (1, 1), (2, 2), (3, 2), (4, 1), (5, 1)], [(1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)], [(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)], [(1, 1), (5, 1), (19, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)], [(0, 1), (3, 1), (16, 2), (31, 1), (32, 1)]]\n",
      "[(0, '0.142*\"health\" + 0.088*\"driving\" + 0.082*\"increased\" + 0.082*\"tension\"'), (1, '0.113*\"good\" + 0.113*\"brocolli\" + 0.101*\"eat\" + 0.096*\"brother\"')]\n",
      "\n",
      "Coherence Score:  0.3374801285549607\n",
      "\n",
      "Coherence Score:  -14.647664242811764\n"
     ]
    }
   ],
   "source": [
    "#Vectorization\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "print(dictionary)\n",
    "for doc in tfidf_corpus:\n",
    "    print(doc)\n",
    "print(bow_corpus)\n",
    "\n",
    "#Model building\n",
    "nmfmodel_tfidf_corpus = Nmf(bow_corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print(nmfmodel_tfidf_corpus.print_topics(num_topics=2, num_words=4))\n",
    "\n",
    "#Evaluating model\n",
    "coherence_model_lda = models.CoherenceModel(model=nmfmodel_tfidf_corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = models.CoherenceModel(model=nmfmodel_tfidf_corpus, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
