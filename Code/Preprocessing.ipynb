{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Documents\\Data\\Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP DUPLICATES\n",
    "# drop_duplicates(self, subset=None, keep=\"first\", inplace=False)\n",
    "# subset:column label or sequence of labels to consider for identifying duplicate rows. \n",
    "#        By default, all the columns are used to find the duplicate rows.\n",
    "# keep:  allowed values are {‘first’, ‘last’, False}, default ‘first’. \n",
    "#        If ‘first’, duplicate rows except the first one is deleted. \n",
    "#        If ‘last’, duplicate rows except the last one is deleted. \n",
    "#        If False, all the duplicate rows are deleted.\n",
    "# inplace:if True, the source DataFrame is changed and None is returned. \n",
    "#        By default, source DataFrame remains unchanged and a new DataFrame instance is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#duplicated() return a series specifying whether whether a row is duplicate or not\n",
    "duplicate_series=df.duplicated()\n",
    "print(type(duplicate_series))\n",
    "sum(duplicate_series)\n",
    "print(duplicate_series.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping based on all columns and remove all occurence of duplicate rows and only keep the first. Default keep=first \n",
    "df2=df.copy(deep=True)\n",
    "df=df.drop_duplicates()\n",
    "print(df2.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping based on all columns and remove all occurences of duplicate rows\n",
    "df=df2.copy(deep=True)\n",
    "df=df.drop_duplicates(keep=False)\n",
    "print(df2.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping based on one column\n",
    "df=df2.copy(deep=True)\n",
    "df=df.drop_duplicates('Hotel_Name')\n",
    "print(df2.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RENAME COLUMNS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.rename(columns={'Hotel_Address':'Hotel_Add','Average_Score':'Avg_Score'},inplace=True)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP COLUMNS, ROWS AND NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns \n",
    "df2=df.copy(deep=True)\n",
    "df.drop(['Hotel_Add'],axis=1,inplace=True)\n",
    "print(df.columns)\n",
    "df=df2.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows \n",
    "index=df[df['Additional_Number_of_Scoring']==244].index\n",
    "df.drop(index,axis=0,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any number of rows\n",
    "df.drop(df.loc[0:10].index, inplace=True) #drops rows from index 0 to 10(including 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop na \n",
    "df=df.dropna(how=\"all\")\n",
    "df=df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD COLUMNS AND ROWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df[['Avg_Score','Hotel_Name']].iloc[0:2]\n",
    "#df2.index=np.arange(0,len(df2)+1) if index starts with 0\n",
    "df2.index=np.arange(0,len(df2))\n",
    "print(type(df2))\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel = [  (34, 'Sydeny' ),\n",
    "             ( 30, 'Delhi' )  ]\n",
    " \n",
    "#create a DataFrame object\n",
    "df_to_be_added = pd.DataFrame(hotel, columns = ['Avg_Score', 'Hotel_Name' ])\n",
    "df_to_be_added.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=df2.append(df_to_be_added,ignore_index=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using dictionary and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.append({'Avg_Score' : 2 , 'Hotel_Name'  : 'Delhi'} , ignore_index=True)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using series and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = df_combined.append(pd.Series([ 21, 'Bangalore'],index=df_combined.columns), ignore_index=True)\n",
    "df_combined = df_combined.append(pd.Series([ 11, 'Bangalore'],index=df_combined.columns), ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using multiple series and append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfSeries = [pd.Series([21, 'India'], index=df_combined.columns ) ,\n",
    "                pd.Series([22,'Japan'], index=df_combined.columns )  ]\n",
    "df_combined = df_combined.append(listOfSeries , ignore_index=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPLACE ROWS, VALUES AND NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing row\n",
    "print(df_combined.head())\n",
    "df_combined.iloc[2] = [ 26, 'India'] #replace the values at index 2\n",
    "print(df_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing a value\n",
    "df_combined.replace(to_replace='Delhi',value='Pune',inplace=True)\n",
    "df_combined.replace(to_replace=['Hotel Arena','Apex Temple Court Hotel'],value='Pune',inplace=True)\n",
    "df_combined.replace('Pune','US',inplace=True)\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing Nan\n",
    "df_combined.replace(to_replace=np.nan,value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversion of data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data type of all columns\n",
    "df=df.astype('str')\n",
    "print(df.dtypes)\n",
    "df2=df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data type of specific columns\n",
    "#using dictionary to convert specific columns \n",
    "convert_dict = {'Additional_Number_of_Scoring':float,\n",
    "                'Total_Number_of_Reviews': int,\n",
    "                'Avg_Score':float\n",
    "               }\n",
    "#'Avg_score': int wont work because if you pass a string representation of a float into int\n",
    "#or a string representation of anything but an integer (including empty string). \n",
    "#If you do want to pass a string representation of a float to an int, you can convert to a float first, then to an integer\n",
    "  \n",
    "df = df.astype(convert_dict) \n",
    "print(df.dtypes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Additional_Number_of_Scoring_quantiles']=pd.qcut(df['Additional_Number_of_Scoring'],q=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[1,2,3]\n",
    "df['Additional_Number_of_Scoring_labels']=pd.qcut(df['Additional_Number_of_Scoring'],q=3,labels=labels)\n",
    "#terciles: q=[0, 1/3, 2/3, 1] or q=3\n",
    "#uintiles: q=[0, .2, .4, .6, .8, 1] or q=5\n",
    "#sextiles: q=[0, 1/6, 1/3, .5, 2/3, 5/6, 1] or q=6\n",
    "#df['Additional_Number_of_Scoring_labels']=pd.qcut(df['Additional_Number_of_Scoring'],q=[0, .2, .4, .6, .8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0,df['Additional_Number_of_Scoring'].max(),10)\n",
    "labels=  np.arange(1,len(bins),1)\n",
    "df['Additional_Number_of_Scoring_binned']=pd.cut(df['Additional_Number_of_Scoring'],bins=bins,labels=labels,include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Additional_Number_of_Scoring','Additional_Number_of_Scoring_quantiles','Additional_Number_of_Scoring_labels','Additional_Number_of_Scoring_binned']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NORMALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Insurance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking normal distribution of variables\n",
    "print(\"Skewness is \",df['charges'].skew())\n",
    "stats.probplot(df['charges'],dist='norm',plot=plt)\n",
    "plt.show()\n",
    "sns.distplot(df['charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MULTICOLLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()  # or by other means, like plt.subplots\n",
    "#gcf() allows you to get a reference to the current figure when using pyplot\n",
    "figsize = fig.get_size_inches()\n",
    "fig.set_size_inches(figsize * 1.5)\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().unstack().sort_values(ascending=False).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df.drop(['sex','smoker','region'],axis=1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df_copy.values, i) for i in range(df_copy.shape[1])]\n",
    "vif[\"features\"] = df_copy.columns\n",
    "vif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy=df_copy.drop(['age'],axis=1)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df_copy.values, i) for i in range(df_copy.shape[1])]\n",
    "vif[\"features\"] = df_copy.columns\n",
    "vif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DUMMIES CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"loan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loan_ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Married</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>Education</th>\n",
       "      <th>Self_Employed</th>\n",
       "      <th>ApplicantIncome</th>\n",
       "      <th>CoapplicantIncome</th>\n",
       "      <th>LoanAmount</th>\n",
       "      <th>Loan_Amount_Term</th>\n",
       "      <th>Credit_History</th>\n",
       "      <th>Property_Area</th>\n",
       "      <th>Loan_Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LP001002</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>5849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LP001003</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>4583</td>\n",
       "      <td>1508.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Rural</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LP001005</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LP001006</td>\n",
       "      <td>Male</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>Not Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>2583</td>\n",
       "      <td>2358.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LP001008</td>\n",
       "      <td>Male</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>Graduate</td>\n",
       "      <td>No</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Urban</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
       "0  LP001002   Male      No          0      Graduate            No   \n",
       "1  LP001003   Male     Yes          1      Graduate            No   \n",
       "2  LP001005   Male     Yes          0      Graduate           Yes   \n",
       "3  LP001006   Male     Yes          0  Not Graduate            No   \n",
       "4  LP001008   Male      No          0      Graduate            No   \n",
       "\n",
       "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
       "0             5849                0.0         NaN             360.0   \n",
       "1             4583             1508.0       128.0             360.0   \n",
       "2             3000                0.0        66.0             360.0   \n",
       "3             2583             2358.0       120.0             360.0   \n",
       "4             6000                0.0       141.0             360.0   \n",
       "\n",
       "   Credit_History Property_Area Loan_Status  \n",
       "0             1.0         Urban           Y  \n",
       "1             1.0         Rural           N  \n",
       "2             1.0         Urban           Y  \n",
       "3             1.0         Urban           Y  \n",
       "4             1.0         Urban           Y  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Gender']=train['Gender'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "      <th>nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female  Male  nan\n",
       "0       0     1    0\n",
       "1       0     1    0\n",
       "2       0     1    0\n",
       "3       0     1    0\n",
       "4       0     1    0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "binz = LabelBinarizer()\n",
    "binz_transform=binz.fit_transform(train['Gender'])\n",
    "binz_transform_df = pd.DataFrame(binz_transform,columns=binz.classes_)\n",
    "binz_transform_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Transformation on test\n",
    "# binz_transform_test=binz.transform(test['Gender'])\n",
    "# binz_transform_df_test = pd.DataFrame(binz_transform_test, columns = binz.classes_)\n",
    "# binz_transform_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCALING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"insurance.csv\")\n",
    "df.drop(['sex','smoker','region'],axis=1,inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:3].values\n",
    "y=df.iloc[:,3].values\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_df=pd.DataFrame(X_train)\n",
    "X_test_df=pd.DataFrame(X_test)\n",
    "print(X_Train_df.head(5))\n",
    "print(X_test_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mx=MinMaxScaler()\n",
    "# X_train=mx.fit_transform(X_train)\n",
    "# X_test=mx.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPUTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Titanic data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum(axis=0)\n",
    "#df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_table(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : 'Percentage of Total Values'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        'Percentage of Total Values', ascending=False).round(1)\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_missing_value_columns=missing_values_table(df)\n",
    "\n",
    "def df_with_half_missing_value_columns(mis_val_table_ren_columns):\n",
    "    mis_val_table_ren_columns_2 = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] >= 50].sort_values(\n",
    "        'Percentage of Total Values', ascending=False).round(1)\n",
    "    return mis_val_table_ren_columns_2\n",
    "\n",
    "df_with_half_missing_value_columns=drop_columns_with_half_nan(df_with_missing_value_columns)\n",
    "\n",
    "def drop_df_with_half_missing_value_columns(df_with_half_missing_value_columns, df):\n",
    "    for i in df_with_half_missing_value_columns.index:\n",
    "        df.drop([i],axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "df_without_half_missing_value_columns=drop_df_with_half_missing_value_columns(df_with_half_missing_value_columns,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_half_missing_value_columns.isnull().sum(axis=0)\n",
    "#df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dropna(how=\"all\").shape\n",
    "#df=df.dropna(how=\"all\")\n",
    "\n",
    "#df.dropna(how=\"all\").shape\n",
    "#df=df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imp=SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "# df['Unnamed: 0']=imp.fit_transform(df[['Unnamed: 0']]).ravel()\n",
    "\n",
    "# imp=SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "# df['Unnamed: 0']=imp.fit_transform(df[['Unnamed: 0']]).ravel()\n",
    "\n",
    "# imp=SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "# df['Unnamed: 0']=imp.fit_transform(df[['Unnamed: 0']]).ravel()\n",
    "\n",
    "# imp=SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "# df['Age']=imp.fit_transform(df[['Age']]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_Embarked_null=df_without_half_missing_value_columns[df_without_half_missing_value_columns['Embarked'].notnull()]\n",
    "df_with_Embarked_null=df_without_half_missing_value_columns[df_without_half_missing_value_columns['Embarked'].isnull()]\n",
    "df_without_Embarked_and_Age_null=df_without_Embarked_null[df_without_Embarked_null['Age'].notnull()]\n",
    "#fit a model on df_without_Embarked_and_Age_null and predict on df_with_Embarked_null \n",
    "#then concat df_with_Embarked_null and df_without_Embarked_null to get a dataframe with no embarked null but with Age null \n",
    "#As of now we are not fitting model and concatenating dataframe to treat Embarked missing values\n",
    "#We will just just Simple Imputer to import missing values in Embarked column \n",
    "#Then fit the model to impute Age missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "df_without_half_missing_value_columns['Embarked']=imp.fit_transform(df_without_half_missing_value_columns[['Embarked']]).ravel()\n",
    "df_without_Age_null=df_without_half_missing_value_columns[df_without_half_missing_value_columns['Age'].notnull()]\n",
    "df_with_Age_null=df_without_half_missing_value_columns[df_without_half_missing_value_columns['Age'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit a model on df_without_Age_null and predict on df_with_Age_null \n",
    "#then concat df_with_Age_null and df_without_Age_null to get a dataframe with no Age null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER TREATMENT\n",
    "\n",
    "#sort the values and find the median i.e the middle value of entire column, \n",
    "#now the find the ist quartile i.e middle value of ist part of column\n",
    "#now the find the ist quartile i.e middle value of iind part of column\n",
    "#now find interquartile range i.e q3-q1\n",
    "#outliers are values which are greater than upper_bound(q3+1.5(interquartile range)) \n",
    "#outliers are values which are less than lower_bound(q1-1.5(interquartile range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston=load_boston()\n",
    "print(boston.data.shape)\n",
    "print(boston.feature_names)\n",
    "df=pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outliers(df,column_name):\n",
    "    for j in column_name:\n",
    "        Q3=df[j].quantile(0.75)\n",
    "        Q1=df[j].quantile(0.25)\n",
    "        IQR = df[j].quantile(0.75) - df[j].quantile(0.25)\n",
    "        H=1.5*IQR\n",
    "        Upper_Whiskar = Q3+H\n",
    "        Lower_Whiskar = Q1-H\n",
    "        Q95 = df[j].quantile(0.95)\n",
    "        Q05 = df[j].quantile(0.05)\n",
    "    \n",
    "        print('Q3 for ',j, ': ', Q3)\n",
    "        print('Q1 for ',j,': ', Q1)\n",
    "        print('IQR for ',j,': ', IQR)\n",
    "        print('H for ',j,': ', H)\n",
    "        print('Upper Whiskar for ',j,': ', Upper_Whiskar)\n",
    "        print('Lower Whiskar for ',j,': ', Lower_Whiskar)\n",
    "    \n",
    "        print('95th Quantile for ',j,': ', Q95)\n",
    "        print('5th Quantile for',j,': ', Q05)\n",
    "        ##df3[j] = df[j].clip(lower=Lower_Whiskar,upper = Upper_Whiskar, inplace=False)\n",
    "        ##df[j].clip(lower=Lower_Whiskar,upper = Upper_Whiskar, inplace=True)\n",
    "        for i in df[j].index.tolist():\n",
    "            if(df.at[i,j]<Lower_Whiskar):\n",
    "                df.at[i,j]= Q05\n",
    "            if(df.at[i,j]>Upper_Whiskar):\n",
    "                df.at[i,j]=Q95\n",
    "\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outliers(df,['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['CRIM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATE TREATMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Online Retail.csv',encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[5:541909].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.InvoiceDate=pd.to_datetime(df.InvoiceDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.InvoiceDate.dt.date\n",
    "#df.InvoiceDate.dt.time\n",
    "#df.InvoiceDate.dt.year\n",
    "#df.InvoiceDate.dt.month\n",
    "#df.InvoiceDate.dt.day\n",
    "#df.InvoiceDate.dt.hour\n",
    "#df.InvoiceDate.dt.minute\n",
    "#df.InvoiceDate.dt.second\n",
    "#df.InvoiceDate.dt.microsecond\n",
    "#df.InvoiceDate.dt.nanosecond\n",
    "#df.InvoiceDate.dt.week\n",
    "#df.InvoiceDate.dt.weekofyear\n",
    "#df.InvoiceDate.dt.dayofweek\n",
    "#df.InvoiceDate.dt.weekday\n",
    "#df.InvoiceDate.dt.weekday_name\n",
    "#df.InvoiceDate.dt.dayofyear\n",
    "#df.InvoiceDate.dt.quarter\n",
    "#df.InvoiceDate.dt.is_month_start\n",
    "#df.InvoiceDate.dt.is_month_end\n",
    "#df.InvoiceDate.dt.is_quarter_start\n",
    "#df.InvoiceDate.dt.is_quarter_end\n",
    "#df.InvoiceDate.dt.is_year_start\n",
    "#df.InvoiceDate.dt.is_year_end\n",
    "#df.InvoiceDate.dt.daysinmonth\n",
    "#df.InvoiceDate.dt.days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['InvoiceDateDiff']=365-(df.InvoiceDate.dt.dayofyear)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
