{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from gensim import corpora, models\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\"\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['brocolli', 'good', 'eat', 'brother', 'like', 'eat', 'good', 'brocolli', 'mother'], ['mother', 'spends', 'lot', 'time', 'driving', 'brother', 'around', 'baseball', 'practice'], ['health', 'expert', 'suggest', 'driving', 'may', 'cause', 'increased', 'tension', 'blood', 'pressure'], ['often', 'feel', 'pressure', 'perform', 'well', 'school', 'mother', 'never', 'seems', 'drive', 'brother', 'better'], ['health', 'professional', 'say', 'brocolli', 'good', 'health']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts = []\n",
    "for i in doc_set:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n",
    "    texts.append(lemmatized_tokens)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc2bow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization\n",
    "#word 0 appeared 2 times, word 1 appeared 1 time\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(dictionary)\n",
    "print(bow_corpus)\n",
    "\n",
    "#Model building\n",
    "ldamodel_bow_corpus = gensim.models.ldamodel.LdaModel(bow_corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "#corpus (iterable of list of (int, float), optional) – Stream of document vectors or sparse matrix of shape \n",
    "# (num_documents, num_terms). If you have a CSC in-memory matrix, you can convert it to a streamed corpus with the help of \n",
    "# gensim.matutils.Sparse2Corpus. If not given, the model is left untrained (presumably because you want to call update() manually)\n",
    "# num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
    "# id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) – Mapping from word IDs to words. \n",
    "# It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
    "# distributed (bool, optional) – Whether distributed computing should be used to accelerate training.\n",
    "# chunksize (int, optional) – Number of documents to be used in each training chunk.\n",
    "# passes (int, optional) – Number of passes through the corpus during training.\n",
    "# update_every (int, optional) – Number of documents to be iterated through for each update. \n",
    "# Set to 0 for batch learning, > 1 for online iterative learning.\n",
    "# alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of length equal to the number of expected topics that \n",
    "# expresses our a-priori belief for each topics’ probability. Alternatively default prior selecting strategies can be employed \n",
    "# by supplying a string\n",
    "# ’symmetric’: Default; uses a fixed symmetric prior per topic,\n",
    "# ’asymmetric’: Uses a fixed normalized asymmetric prior of 1.0 / (topic_index + sqrt(num_topics)),\n",
    "# ’auto’: Learns an asymmetric prior from the corpus (not available if distributed==True).\n",
    "# eta ({float, np.array, str}, optional) –\n",
    "# A-priori belief on word probability, this can be:\n",
    "# scalar for a symmetric prior over topic/word probability,\n",
    "# vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "# matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "# the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "# decay (float, optional) – A number between (0.5, 1] to weight what percentage of the previous lambda value is \n",
    "# forgotten when each new document is examined. Corresponds to Kappa from Matthew D. Hoffman, David M. Blei, \n",
    "# Francis Bach: “Online Learning for Latent Dirichlet Allocation NIPS‘10”.\n",
    "# offset (float, optional) – Hyper-parameter that controls how much we will slow down the first steps the first few iterations. \n",
    "# Corresponds to Tau_0 from Matthew D. Hoffman, David M. Blei, Francis Bach: “Online Learning for Latent Dirichlet Allocation NIPS‘10”.\n",
    "# eval_every (int, optional) – Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
    "# iterations (int, optional) – Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
    "# gamma_threshold (float, optional) – Minimum change in the value of the gamma parameters to continue iterating.\n",
    "# minimum_probability (float, optional) – Topics with a probability lower than this threshold will be filtered out.\n",
    "# random_state ({np.random.RandomState, int}, optional) – Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
    "# ns_conf (dict of (str, object), optional) – Key word parameters propagated to gensim.utils.getNS() to get a Pyro4 Nameserved. \n",
    "# Only used if distributed is set to True.\n",
    "# minimum_phi_value (float, optional) – if per_word_topics is True, this represents a lower bound on the term probabilities.\n",
    "# per_word_topics (bool) – If True, the model also computes a list of topics,sorted in descending order of most likely topics \n",
    "# for each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
    "# callbacks (list of Callback) – Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
    "# dtype ({numpy.float16, numpy.float32, numpy.float64}, optional) – Data-type to use during calculations inside model. \n",
    "# All inputs are also converted.\n",
    "\n",
    "print(ldamodel_bow_corpus.print_topics(num_topics=2, num_words=4))\n",
    "\n",
    "#Evaluating model\n",
    "coherence_model_lda = models.CoherenceModel(model=ldamodel_bow_corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = models.CoherenceModel(model=ldamodel_bow_corpus, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.40784451109112935), (1, 0.11368521994734913), (2, 0.7163669735975946), (3, 0.40784451109112935), (4, 0.3581834867987973), (5, 0.11368521994734913)]\n",
      "[(1, 0.12424759593709131), (5, 0.12424759593709131), (6, 0.3914619434234833), (7, 0.3914619434234833), (8, 0.2228684610131362), (9, 0.3914619434234833), (10, 0.3914619434234833), (11, 0.3914619434234833), (12, 0.3914619434234833)]\n",
      "[(8, 0.2016345105176491), (13, 0.35416512946544426), (14, 0.35416512946544426), (15, 0.35416512946544426), (16, 0.2016345105176491), (17, 0.35416512946544426), (18, 0.35416512946544426), (19, 0.2016345105176491), (20, 0.35416512946544426), (21, 0.35416512946544426)]\n",
      "[(1, 0.10283764444679584), (5, 0.10283764444679584), (19, 0.18446447498008656), (22, 0.32400646345397865), (23, 0.32400646345397865), (24, 0.32400646345397865), (25, 0.32400646345397865), (26, 0.32400646345397865), (27, 0.32400646345397865), (28, 0.32400646345397865), (29, 0.32400646345397865), (30, 0.32400646345397865)]\n",
      "[(0, 0.2866473576676298), (3, 0.2866473576676298), (16, 0.5732947153352596), (31, 0.5034877128853272), (32, 0.5034877128853272)]\n",
      "[(0, '0.036*\"driving\" + 0.035*\"practice\" + 0.035*\"lot\" + 0.035*\"time\"'), (1, '0.057*\"eat\" + 0.056*\"brocolli\" + 0.056*\"good\" + 0.052*\"health\"')]\n",
      "\n",
      "Coherence Score:  0.298873944678119\n",
      "\n",
      "Coherence Score:  -16.253411403680605\n"
     ]
    }
   ],
   "source": [
    "#Vectorization\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "print(dictionary)\n",
    "for doc in tfidf_corpus:\n",
    "    print(doc)\n",
    "\n",
    "\n",
    "#Model Building\n",
    "ldamodel_tfidf_corpus = gensim.models.ldamodel.LdaModel(tfidf_corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "print(ldamodel_tfidf_corpus.print_topics(num_topics=2, num_words=4))\n",
    "\n",
    "#Evaluating model\n",
    "coherence_model_lda = models.CoherenceModel(model=ldamodel_tfidf_corpus, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "coherence_model_lda = models.CoherenceModel(model=ldamodel_tfidf_corpus, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
