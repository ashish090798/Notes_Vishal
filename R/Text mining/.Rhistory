install.packages('caret')
library(caret)
library(forecast)
install.packages("forecast")
install.packages('TTR')
install.packages('fpp2')
data(iris)
head(iris)
data <- iris[, c(1, 2, 5)]
head(data)
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
str(data)
table(data$Species)
install.packages('DMwR')
library(DMwR)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=200)
?SMOTE
head(newData)
table(newData$Species)
# ROSE (Random Over Sampling Examples) package -
# Helps in implementing techniques for handling class imbalance
install.packages("ROSE")
library(ROSE)
data(hacide)
head(hacide.train)
str(hacide.train)
# Check for data imbalance
table(hacide.train$cls)
prop.table(table(hacide.train$cls))
# Lets build the logistic regression model on this data
# (without balancing the data)
classifier_imb = glm(cls ~ ., data = hacide.train, family = 'binomial')
summary(classifier_imb)
pred_classifier_imb = predict(classifier_imb, type = 'response', newdata = hacide.test)
y_pred = pred_classifier_imb > 0.5
print(y_pred)
accuracy.meas(hacide.test$cls, pred_classifier_imb)
# Lets also check the model's performance using ROC
# We use the roc.curve function from ROSE for this
roc.curve(hacide.test$cls, pred_classifier_imb)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 1960)$data
table(data_balanced_over$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 40)$data
table(data_balanced_under$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
?ovun.sample
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 60)$data
table(data_balanced_under$cls)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 2000)$data
table(data_balanced_over$cls)
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 90)$data
table(data_balanced_under$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.5)$data
table(data_balanced_both$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.1)$data
table(data_balanced_both$cls)
###### Synthetic Data Generation (ROSE) ######
# Lets now generate synthetic data using ROSE
data_balanced_rose <- ROSE(cls ~ ., data = hacide.train, p = 0.5)$data
table(data_balanced_rose$cls)
setwd("~/Data/Text Mining/r")
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(tm) ### text mining
library(NLP) ## Natural language processing
#how to read a text file
posText = read.delim(file='polarity_pos (1).txt', header=FALSE, stringsAsFactors=FALSE)
posText = posText$V1
head(posText)
class(posText)
posText = unlist(lapply(posText, function(x) { str_split(x, "\n") }))
posText[ 1:2]
score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores <- laply(sentences, function(sentence, pos.words, neg.words){
sentence <- gsub('[[:punct:]]', "", sentence)
sentence <- gsub('[[:cntrl:]]', "", sentence)
sentence <- gsub('\\d+', "", sentence)
sentence <- tolower(sentence)
word.list <- str_split(sentence, '\\s+')
words <- unlist(word.list)
pos.matches <- match(words, pos.words)
neg.matches <- match(words, neg.words)
pos.matches <- !is.na(pos.matches)
neg.matches <- !is.na(neg.matches)
score <- sum(pos.matches)  - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress)
scores.df <- data.frame(score=scores, text=sentences)
return(scores.df)
}
#load up word polarity list and format it
afinn_list = read.delim(file='AFINN-111.txt', header=FALSE, stringsAsFactors=FALSE)
head(afinn_list)
names(afinn_list) = c('word', 'score')
afinn_list$word = tolower(afinn_list$word)
head(afinn_list)
negterms = c(afinn_list$word[afinn_list$score < 0 ] )
posterms = c(afinn_list$word[afinn_list$score > 0 ] )
posterms[1:2]
negterms[80:90]
#build tables of positive and negative sentences with scores
posResult = as.data.frame(score.sentiment(posText, posterms,   negterms))
head(posResult)
hist(posResult$score)
head(posResult)
write.csv(posResult,"results.csv", row.names = F)
getwd()
score.sentiment("this is a awesome phone, but poor battery", posterms, negterms)
names(posResult)
head(posResult)
hist(posResult$score)
hist(posResult$score)
table(posResult$score)
head(posResult)
posResult
table(posResult$score)
table(negResult$score)
### subset all the reviews where the score is less than -2
negativereviews = posResult[posResult$score <= -1 ,]
library(tm)
pos_corpus <- Corpus(VectorSource(negativereviews$text))
list_stop = c("movie", "film")
# clean up the corpus using tm_map()
corpus_clean <- tm_map(pos_corpus, tolower)
corpus_clean =  tm_map(corpus_clean, PlainTextDocument)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords("english"))
corpus_clean <- tm_map(corpus_clean, removeWords, list_stop)
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)
### create a new list of stop words
list_stop = c("movie","movies","film","cinema","picture","can")
new_list_stop = c( list_stop,stopwords("english"))
### creating Term Document matrix
dtm_pos = DocumentTermMatrix(corpus_clean)
tdm_pos = TermDocumentMatrix(corpus_clean)
