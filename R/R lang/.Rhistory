install.packages('caret')
library(caret)
library(forecast)
install.packages("forecast")
install.packages('TTR')
install.packages('fpp2')
data(iris)
head(iris)
data <- iris[, c(1, 2, 5)]
head(data)
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
str(data)
table(data$Species)
install.packages('DMwR')
library(DMwR)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=200)
?SMOTE
head(newData)
table(newData$Species)
# ROSE (Random Over Sampling Examples) package -
# Helps in implementing techniques for handling class imbalance
install.packages("ROSE")
library(ROSE)
data(hacide)
head(hacide.train)
str(hacide.train)
# Check for data imbalance
table(hacide.train$cls)
prop.table(table(hacide.train$cls))
# Lets build the logistic regression model on this data
# (without balancing the data)
classifier_imb = glm(cls ~ ., data = hacide.train, family = 'binomial')
summary(classifier_imb)
pred_classifier_imb = predict(classifier_imb, type = 'response', newdata = hacide.test)
y_pred = pred_classifier_imb > 0.5
print(y_pred)
accuracy.meas(hacide.test$cls, pred_classifier_imb)
# Lets also check the model's performance using ROC
# We use the roc.curve function from ROSE for this
roc.curve(hacide.test$cls, pred_classifier_imb)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 1960)$data
table(data_balanced_over$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 40)$data
table(data_balanced_under$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
?ovun.sample
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 60)$data
table(data_balanced_under$cls)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 2000)$data
table(data_balanced_over$cls)
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 90)$data
table(data_balanced_under$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.5)$data
table(data_balanced_both$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.1)$data
table(data_balanced_both$cls)
###### Synthetic Data Generation (ROSE) ######
# Lets now generate synthetic data using ROSE
data_balanced_rose <- ROSE(cls ~ ., data = hacide.train, p = 0.5)$data
table(data_balanced_rose$cls)
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb", tuneGrid=grid)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
install.packages(klaR)
install.packages('klaR')
library(klaR)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
library(caret)
library(psych)
install.packages('caret')
install.packages("caret")
library(psych)
install.packages('pysch')
# read in the data
data <- sat.act
library(psych)
install.packages('pysch')
> library(dummies)
library(dummies)
install.packages("dummies")
setwd("~/Data/Pre processing and Visualization")
students <- read.csv("Churn_MV.csv")
students <- read.csv("Churn_MV.csv")
library(dummies)
str(students)
ncols(students)
ncol(students)
student=students[:,c(20)]
student=students[:,20]
student=students(:,20)
student=students[:,c(20,22)]
student=students[,c(20,22)]
student=students[,20]
students.new <- dummy.data.frame(student, sep = ".")
students.new <- dummy.data.frame(student)
students.new <- dummy.data.frame(students)
students <- read.csv("income_Census.csv")
str(income_Census)
str(df)
str(df)
df <- read.csv("income_Census.csv")
str(df)
library(dummies)
dfnew <- dummy.data.frame(df, sep = ".")
> names(students.new)
names(students.new)
names(dfnew)
