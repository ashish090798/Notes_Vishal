install.packages('caret')
library(caret)
library(forecast)
install.packages("forecast")
install.packages('TTR')
install.packages('fpp2')
data(iris)
head(iris)
data <- iris[, c(1, 2, 5)]
head(data)
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
str(data)
table(data$Species)
install.packages('DMwR')
library(DMwR)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=200)
?SMOTE
head(newData)
table(newData$Species)
# ROSE (Random Over Sampling Examples) package -
# Helps in implementing techniques for handling class imbalance
install.packages("ROSE")
library(ROSE)
data(hacide)
head(hacide.train)
str(hacide.train)
# Check for data imbalance
table(hacide.train$cls)
prop.table(table(hacide.train$cls))
# Lets build the logistic regression model on this data
# (without balancing the data)
classifier_imb = glm(cls ~ ., data = hacide.train, family = 'binomial')
summary(classifier_imb)
pred_classifier_imb = predict(classifier_imb, type = 'response', newdata = hacide.test)
y_pred = pred_classifier_imb > 0.5
print(y_pred)
accuracy.meas(hacide.test$cls, pred_classifier_imb)
# Lets also check the model's performance using ROC
# We use the roc.curve function from ROSE for this
roc.curve(hacide.test$cls, pred_classifier_imb)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 1960)$data
table(data_balanced_over$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 40)$data
table(data_balanced_under$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
?ovun.sample
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 60)$data
table(data_balanced_under$cls)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 2000)$data
table(data_balanced_over$cls)
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 90)$data
table(data_balanced_under$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.5)$data
table(data_balanced_both$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.1)$data
table(data_balanced_both$cls)
###### Synthetic Data Generation (ROSE) ######
# Lets now generate synthetic data using ROSE
data_balanced_rose <- ROSE(cls ~ ., data = hacide.train, p = 0.5)$data
table(data_balanced_rose$cls)
#Program : randomForest with Crossvalidation and Gridsearch
#### Random Forest and Boosting
chrn = read.csv("churn.csv")
setwd("~/Data/Decision tree and Random forest")
#Program : randomForest with Crossvalidation and Gridsearch
#### Random Forest and Boosting
chrn = read.csv("churn.csv")
chrn = chrn[,-c(19:21)]
chrn$Churn = as.factor(chrn$Churn)
chrn$Churn = as.factor(chrn$Churn)
ids = sample(nrow(chrn), nrow(chrn)*0.8)
train = chrn[ids,]
test = chrn[-ids,]
library(randomForest)
unique(train$Churn)
rftree = randomForest( Churn ~ . , data = train, ntree = 70, mtry = 4, max_depth = 8, nodesize = 5 )
test$pred = predict(rftree, newdata = test)
test
View(test)
table(test$Churn, test$pred)
