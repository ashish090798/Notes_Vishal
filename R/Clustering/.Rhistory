install.packages('caret')
library(caret)
library(forecast)
install.packages("forecast")
install.packages('TTR')
install.packages('fpp2')
data(iris)
head(iris)
data <- iris[, c(1, 2, 5)]
head(data)
data$Species <- factor(ifelse(data$Species == "setosa","rare","common"))
str(data)
table(data$Species)
install.packages('DMwR')
library(DMwR)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=100)
table(newData$Species)
newData <- SMOTE(Species ~ ., data, perc.over = 600,perc.under=200)
?SMOTE
head(newData)
table(newData$Species)
# ROSE (Random Over Sampling Examples) package -
# Helps in implementing techniques for handling class imbalance
install.packages("ROSE")
library(ROSE)
data(hacide)
head(hacide.train)
str(hacide.train)
# Check for data imbalance
table(hacide.train$cls)
prop.table(table(hacide.train$cls))
# Lets build the logistic regression model on this data
# (without balancing the data)
classifier_imb = glm(cls ~ ., data = hacide.train, family = 'binomial')
summary(classifier_imb)
pred_classifier_imb = predict(classifier_imb, type = 'response', newdata = hacide.test)
y_pred = pred_classifier_imb > 0.5
print(y_pred)
accuracy.meas(hacide.test$cls, pred_classifier_imb)
# Lets also check the model's performance using ROC
# We use the roc.curve function from ROSE for this
roc.curve(hacide.test$cls, pred_classifier_imb)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 1960)$data
table(data_balanced_over$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 40)$data
table(data_balanced_under$cls)
###### Undersampling ######
# Lets now apply undersampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
?ovun.sample
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 60)$data
table(data_balanced_under$cls)
# Lets now apply oversampling for balancing the data
# We use the ovun.sample function from ROSE
# N is the number of data points in the resulting balanced data
data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 2000)$data
table(data_balanced_over$cls)
data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 90)$data
table(data_balanced_under$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.5)$data
table(data_balanced_both$cls)
###### Undersampling & Oversampling ######
# We can apply both these techniques for balancing the data
# p is the approx probability of the positive class in the resulting balanced data
data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",p = 0.1)$data
table(data_balanced_both$cls)
###### Synthetic Data Generation (ROSE) ######
# Lets now generate synthetic data using ROSE
data_balanced_rose <- ROSE(cls ~ ., data = hacide.train, p = 0.5)$data
table(data_balanced_rose$cls)
# load the library
library(caret)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="cv", number=10)
# fix the parameters of the algorithm
grid <- expand.grid(.fL=c(0), .usekernel=c(FALSE))
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb", tuneGrid=grid)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# define training control
train_control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
install.packages(klaR)
install.packages('klaR')
library(klaR)
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
# load the iris dataset
data(iris)
# define training control
train_control <- trainControl(method="LOOCV")
# train the model
model <- train(Species~., data=iris, trControl=train_control, method="nb")
# summarize results
print(model)
library(caret)
library(psych)
install.packages('caret')
install.packages("caret")
library(psych)
install.packages('pysch')
# read in the data
data <- sat.act
library(psych)
install.packages('pysch')
data1 = read.csv("universalBank.csv")
setwd("~/Data/Clustering")
data1 = read.csv("universalBank.csv")
data1 = data1[data1$Personal.Loan == 1,]
data1$Personal.Loan = NULL
data1$ZIP.Code  = NULL
data1$ID = NULL
# data1$Family = as.factor(data1$Family)
data1$Education = as.factor(data1$Education)
summary(data1)
View(cor(data1[,-c(4,6,8,9,10,11)]))
data1$Experience = NULL # High cor with Age
### Convert Cat to numeric
library(dummies)
data1_dummies = dummy.data.frame(data1)
names(data1_dummies)
data1_dummies = data1_dummies[,-c(7)]
summary(data1_dummies)
### Scaling
###Min max scaling
fnScaling = function(x){
return((x-min(x))/(max(x)-min(x)))
}
for(i in 1:ncol(data1_dummies)){
data1_dummies[,i] = fnScaling(data1_dummies[,i])
}
summary(data1_dummies)
### Kmeans clustering
clust =  kmeans(x=data1_dummies,centers = 6)### Only numeric data to be passed
clust$centers
data1$clust = clust$cluster
View(data1)
clust$cluster ## CLUSTER ID
clust$centers ## Centroids
clust$betweenss ##
clust$withinss
mean(clust$withinss)/clust$betweenss ## IntraCluster/interCluster
data1_dummies$clust = NULL
summary(data1_dummies)
## Identify Best number of clusters
withinByBetween = c()
for(i in 2:15){
clust = kmeans(x=data1,centers = i)
##betweenByTotal = c(betweenByTotal,clust$betweenss/clust$totss)
withinByBetween = c(withinByBetween, mean(clust$withinss)/clust$betweenss)
}
plot(2:15,withinByBetween,type = 'l')
clust =  kmeans(x=data1,centers = 6)
clust$centers
data1$cluster = clust$cluster
View(data1_dummies[data1_dummies$cluster < 3,])
